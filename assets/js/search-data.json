{
  
    
        "post0": {
            "title": "Reading in a Wave File",
            "content": "First I created the simple sound file in Reaper. I could not use the base WAV (Waveform Audio File Format) file because it stores it as 24-bit data which does not work with the scipy wavfile method. However, choosing 32-bit data is a simple option when you render the sound. . The sound I created was simple 2 seconds of a sine wave at around C4. Zooming in to a 0.045 second window we can see the wave in Reaper. A simple peak-to-peak measure was 0.00375 seconds long which gives a frequency around 267 herz. Since this was a C4, which is typically 262 it is not too far off. . To read the file a quick search brought up the wavfile method from scipy. I basically follow the example provided there on my sound. . from scipy.io import wavfile from pathlib import Path . p = Path(&#39;sounds&#39;) . q = p / &#39;simple_c4.wav&#39; . q . WindowsPath(&#39;sounds/simple_c4.wav&#39;) . sample_rate, data = wavfile.read(q) . c: python37 lib site-packages scipy io wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it. WavFileWarning) . sample_rate . 44100 . data . array([[ 0.0000000e+00, 0.0000000e+00], [ 0.0000000e+00, 0.0000000e+00], [-3.8028106e-06, -3.8028106e-06], ..., [-6.6743125e-03, -6.6743125e-03], [-4.4752425e-03, -4.4752425e-03], [-2.2473824e-03, -2.2473824e-03]], dtype=float32) . data.shape . (88200, 2) . Since the recording is 2 seconds long and the sample rate is 44100, there are 88200 samples total, as expected. This is a list of channel lists. So, the stereo data is recorded for each sample. . length = data.shape[0] / sample_rate length . 2.0 . import numpy as np import matplotlib.pyplot as plt . time = np.linspace(0., length, data.shape[0]) . In this case the channels are the same, so just plotting the first one: . plt.plot(time, data[:, 0]) plt.xlabel(&quot;Time [s]&quot;) plt.ylabel(&quot;Amplitude&quot;) plt.show() . Too much info for this tiny graph! Let&#39;s look at just one second and then zoom in more. . span_length = data.shape[0] // 2 . span_length . 44100 . plt.plot(time[:span_length], data[:span_length, 0]) plt.xlabel(&quot;Time [s]&quot;) plt.ylabel(&quot;Amplitude&quot;) plt.show() . span_length = data.shape[0] // 20 . span_length . 4410 . plt.plot(time[:span_length], data[:span_length, 0]) plt.xlabel(&quot;Time [s]&quot;) plt.ylabel(&quot;Amplitude&quot;) plt.show() . span_length = data.shape[0] // 200 . span_length . 441 . start_sample = 500 . plt.plot(time[start_sample:start_sample+span_length], data[start_sample:start_sample+span_length, 0]) plt.xlabel(&quot;Time [s]&quot;) plt.ylabel(&quot;Amplitude&quot;) plt.show() . A simple sine wave! .",
            "url": "http://simonstolarczyk.com/jupyter/2020/09/30/Reading_in_a_Wave_File.html",
            "relUrl": "/jupyter/2020/09/30/Reading_in_a_Wave_File.html",
            "date": " ‚Ä¢ Sep 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Trying Out HuggingFace",
            "content": "Updated Blog Posting Method . After going through the pain of converting a Notebook to a markdown file and then editing that markdown file to look nice (in my last post), I saw that there was a better way to hand that process: fastpages. The process was slightly rocky, but I finally think I have things more or less figured out, including linking it to a domain under my name! . As a first test of the capability of uploading a notebook to a blog post, I am going to toy with the Hugging Face models. Interesting name for a company/group, with lots of Alien vibes. I saw this super cool tweet: No labeled data? No problem.The ü§ó Transformers master branch now includes a built-in pipeline for zero-shot text classification, to be included in the next release.Try it out in the notebook here: https://t.co/31Z6LR1NjK pic.twitter.com/IugPUj3sa1 . &mdash; Hugging Face (@huggingface) August 11, 2020 . Per the instructions here I made a virtual environment to try out some transformers: . pyenv virtualenv 3.8 hface pyenv activate hface pip install jupyter pip install --upgrade pip pip install torch pip install transformers . And then I tested with: . python -c &quot;from transformers import pipeline; print(pipeline(&#39;sentiment-analysis&#39;)(&#39;I hate you&#39;))&quot; . Which gave a correct sentiment score, I think at least, a negative score close to 1. . from transformers import pipeline . pipeline(&#39;sentiment-analysis&#39;)(&#39;jog off&#39;) . [{&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.905813992023468}] . pipeline(&#39;sentiment-analysis&#39;)(&#39;exactly&#39;) . [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9990326166152954}] . pipeline(&#39;sentiment-analysis&#39;)(&#39;I saw this super cool tweet&#39;) . [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.998775064945221}] . Very cool! . Trying Out Pipelines . I attempted running a zero-shot classifier, but got an error (&quot;Unknown task zero-shot-classification, available tasks are [&#39;feature-extraction&#39;, &#39;sentiment-analysis&#39;, &#39;ner&#39;, &#39;question-answering&#39;, &#39;fill-mask&#39;, &#39;summarization&#39;, &#39;translation_en_to_fr&#39;, &#39;translation_en_to_de&#39;, &#39;translation_en_to_ro&#39;, &#39;text-generation&#39;]&quot;). I guess this is because it is a new feature that hasn&#39;t quite made it to the latest version: . classifer = pipeline(&#39;zero-shot-classification&#39;) . Instead, I will play around with some of the other classifers. . en_to_de_translate = pipeline(&#39;translation_en_to_de&#39;) . . /home/simon/.pyenv/versions/3.8.3/envs/hface/lib/python3.8/site-packages/transformers/modeling_auto.py:796: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models. warnings.warn( . . Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: [&#39;encoder.embed_tokens.weight&#39;, &#39;decoder.embed_tokens.weight&#39;, &#39;lm_head.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . en_to_de_translate(&quot;no&quot;) . [{&#39;translation_text&#39;: &#39;nein, nein, nein, nein!&#39;}] . Checks out. Let&#39;s see if some other stuff accords with my degrading knowledge of German: . en_to_de_translate(&quot;this is my room&quot;) . [{&#39;translation_text&#39;: &#39;das ist mein Raum.&#39;}] . I probably would have used Zimmer instead of Raum, since Raum is more &quot;space&quot; than &quot;room&quot; to me. . en_to_de_translate(&quot;monkey, hippo, porcupine, dog, cat, rabbit&quot;) . [{&#39;translation_text&#39;: &#39;Affen, Hippo, Pfauen, Hunde, Katzen, Kaninchen, Hunde, Katzen, Kaninchen.&#39;}] . It looks like it uses the plural for nouns. Hippo didn&#39;t translate to anything different, apparently Flusspferd (water horse) is favored by Leo. I like Stachelschwein (&quot;spike pig&quot;) better for porcupine (which apparently live in Texas now!?) and furthermore Pfauen looks to actual mean peacocks. I&#39;m not sure why dog (Hund), cat (Katze), and rabbit (Kaninchen) are repeated, but those look good. . Thus it&#39;s not perfect, but something that took less than a minute can out-translate my 4-ish years of German classes that I haven&#39;t touched up on in like a decade. Ouch. . Named Entity Recognition . Finally, to cap off this short test post let&#39;s try out the named entity recognition task. They provide an example of the classifier in their docs as well as a short list of what different abbreviations mean: . O, Outside of a named entity | B-MIS, Beginning of a miscellaneous entity right after another miscellaneous entity | I-MIS, Miscellaneous entity | B-PER, Beginning of a person‚Äôs name right after another person‚Äôs name | I-PER, Person‚Äôs name | B-ORG, Beginning of an organisation right after another organisation | I-ORG, Organisation | B-LOC, Beginning of a location right after another location | I-LOC, Location | . ner = pipeline(&#39;ner&#39;) . . First using their example: . sequence = (&quot;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&quot; + &quot;close to the Manhattan Bridge which is visible from the window.&quot;) . from pprint import pprint . for entry in ner(sequence): pprint(entry) . {&#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 1, &#39;score&#39;: 0.9995632767677307, &#39;word&#39;: &#39;Hu&#39;} {&#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 2, &#39;score&#39;: 0.9915938973426819, &#39;word&#39;: &#39;##gging&#39;} {&#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 3, &#39;score&#39;: 0.9982671737670898, &#39;word&#39;: &#39;Face&#39;} {&#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 4, &#39;score&#39;: 0.9994403719902039, &#39;word&#39;: &#39;Inc&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 11, &#39;score&#39;: 0.9994346499443054, &#39;word&#39;: &#39;New&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 12, &#39;score&#39;: 0.9993270635604858, &#39;word&#39;: &#39;York&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 13, &#39;score&#39;: 0.9993864893913269, &#39;word&#39;: &#39;City&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 19, &#39;score&#39;: 0.9825621843338013, &#39;word&#39;: &#39;D&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 20, &#39;score&#39;: 0.9369831085205078, &#39;word&#39;: &#39;##UM&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 21, &#39;score&#39;: 0.8987104296684265, &#39;word&#39;: &#39;##BO&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 29, &#39;score&#39;: 0.9758240580558777, &#39;word&#39;: &#39;Manhattan&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 30, &#39;score&#39;: 0.9902493953704834, &#39;word&#39;: &#39;Bridge&#39;} . Impressive, especially how it recognizes DUMBO as a location. (side note, I actually visited that area in my first trip to NYC last year). . Looking forward to trying out these transformers more in the future! .",
            "url": "http://simonstolarczyk.com/jupyter/2020/08/15/huggingface_test.html",
            "relUrl": "/jupyter/2020/08/15/huggingface_test.html",
            "date": " ‚Ä¢ Aug 15, 2020"
        }
        
    
  

  
  
      ,"page0": {
          "title": "An Example Markdown Post",
          "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
          "url": "http://simonstolarczyk.com/archive/2020-01-14-test-markdown-post.html",
          "relUrl": "/archive/2020-01-14-test-markdown-post.html",
          "date": ""
      }
      
  

  

  
      ,"page2": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "http://simonstolarczyk.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "http://simonstolarczyk.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}