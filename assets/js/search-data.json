{
  
    
        "post0": {
            "title": "Cheatsheet",
            "content": "Common Command and Setup Cheatsheet . terminal shortcuts . ctrl + P = previous command | ctrl + u = erase line before cursor | . general linux shortcuts . ctrl + alt + t = new terminal window | . setting up Vim . sudo apt install vim | TODO: setting jk | .",
            "url": "http://simonstolarczyk.com/2022/02/12/cheatsheet.html",
            "relUrl": "/2022/02/12/cheatsheet.html",
            "date": " • Feb 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Reading in a Wave File",
            "content": "First I created the simple sound file in Reaper. I could not use the base WAV (Waveform Audio File Format) file because it stores it as 24-bit data which does not work with the scipy wavfile method. However, choosing 32-bit data is a simple option when you render the sound. . The sound I created was simple 2 seconds of a sine wave at around C4. Zooming in to a 0.045 second window we can see the wave in Reaper. A simple peak-to-peak measure was 0.00375 seconds long which gives a frequency around 267 herz. Since this was a C4, which is typically 262 it is not too far off. . To read the file a quick search brought up the wavfile method from scipy. I basically follow the example provided there on my sound. . from scipy.io import wavfile from pathlib import Path . p = Path(&#39;sounds&#39;) . q = p / &#39;simple_c4.wav&#39; . q . WindowsPath(&#39;sounds/simple_c4.wav&#39;) . sample_rate, data = wavfile.read(q) . c: python37 lib site-packages scipy io wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it. WavFileWarning) . sample_rate . 44100 . data . array([[ 0.0000000e+00, 0.0000000e+00], [ 0.0000000e+00, 0.0000000e+00], [-3.8028106e-06, -3.8028106e-06], ..., [-6.6743125e-03, -6.6743125e-03], [-4.4752425e-03, -4.4752425e-03], [-2.2473824e-03, -2.2473824e-03]], dtype=float32) . data.shape . (88200, 2) . Since the recording is 2 seconds long and the sample rate is 44100, there are 88200 samples total, as expected. This is a list of channel lists. So, the stereo data is recorded for each sample. . length = data.shape[0] / sample_rate length . 2.0 . import numpy as np import matplotlib.pyplot as plt . time = np.linspace(0., length, data.shape[0]) . In this case the channels are the same, so just plotting the first one: . plt.plot(time, data[:, 0]) plt.xlabel(&quot;Time [s]&quot;) plt.ylabel(&quot;Amplitude&quot;) plt.show() . Too much info for this tiny graph! Let&#39;s look at just one second and then zoom in more. . span_length = data.shape[0] // 2 . span_length . 44100 . plt.plot(time[:span_length], data[:span_length, 0]) plt.xlabel(&quot;Time [s]&quot;) plt.ylabel(&quot;Amplitude&quot;) plt.show() . span_length = data.shape[0] // 20 . span_length . 4410 . plt.plot(time[:span_length], data[:span_length, 0]) plt.xlabel(&quot;Time [s]&quot;) plt.ylabel(&quot;Amplitude&quot;) plt.show() . span_length = data.shape[0] // 200 . span_length . 441 . start_sample = 500 . plt.plot(time[start_sample:start_sample+span_length], data[start_sample:start_sample+span_length, 0]) plt.xlabel(&quot;Time [s]&quot;) plt.ylabel(&quot;Amplitude&quot;) plt.show() . A simple sine wave! .",
            "url": "http://simonstolarczyk.com/jupyter/2020/09/30/Reading_in_a_Wave_File.html",
            "relUrl": "/jupyter/2020/09/30/Reading_in_a_Wave_File.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Trying Out HuggingFace",
            "content": "Updated Blog Posting Method . After going through the pain of converting a Notebook to a markdown file and then editing that markdown file to look nice (in my last post), I saw that there was a better way to hand that process: fastpages. The process was slightly rocky, but I finally think I have things more or less figured out, including linking it to a domain under my name! . As a first test of the capability of uploading a notebook to a blog post, I am going to toy with the Hugging Face models. Interesting name for a company/group, with lots of Alien vibes. I saw this super cool tweet: No labeled data? No problem.The 🤗 Transformers master branch now includes a built-in pipeline for zero-shot text classification, to be included in the next release.Try it out in the notebook here: https://t.co/31Z6LR1NjK pic.twitter.com/IugPUj3sa1 . &mdash; Hugging Face (@huggingface) August 11, 2020 . Per the instructions here I made a virtual environment to try out some transformers: . pyenv virtualenv 3.8 hface pyenv activate hface pip install jupyter pip install --upgrade pip pip install torch pip install transformers . And then I tested with: . python -c &quot;from transformers import pipeline; print(pipeline(&#39;sentiment-analysis&#39;)(&#39;I hate you&#39;))&quot; . Which gave a correct sentiment score, I think at least, a negative score close to 1. . from transformers import pipeline . pipeline(&#39;sentiment-analysis&#39;)(&#39;jog off&#39;) . [{&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.905813992023468}] . pipeline(&#39;sentiment-analysis&#39;)(&#39;exactly&#39;) . [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9990326166152954}] . pipeline(&#39;sentiment-analysis&#39;)(&#39;I saw this super cool tweet&#39;) . [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.998775064945221}] . Very cool! . Trying Out Pipelines . I attempted running a zero-shot classifier, but got an error (&quot;Unknown task zero-shot-classification, available tasks are [&#39;feature-extraction&#39;, &#39;sentiment-analysis&#39;, &#39;ner&#39;, &#39;question-answering&#39;, &#39;fill-mask&#39;, &#39;summarization&#39;, &#39;translation_en_to_fr&#39;, &#39;translation_en_to_de&#39;, &#39;translation_en_to_ro&#39;, &#39;text-generation&#39;]&quot;). I guess this is because it is a new feature that hasn&#39;t quite made it to the latest version: . classifer = pipeline(&#39;zero-shot-classification&#39;) . Instead, I will play around with some of the other classifers. . en_to_de_translate = pipeline(&#39;translation_en_to_de&#39;) . . /home/simon/.pyenv/versions/3.8.3/envs/hface/lib/python3.8/site-packages/transformers/modeling_auto.py:796: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models. warnings.warn( . . Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: [&#39;encoder.embed_tokens.weight&#39;, &#39;decoder.embed_tokens.weight&#39;, &#39;lm_head.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . en_to_de_translate(&quot;no&quot;) . [{&#39;translation_text&#39;: &#39;nein, nein, nein, nein!&#39;}] . Checks out. Let&#39;s see if some other stuff accords with my degrading knowledge of German: . en_to_de_translate(&quot;this is my room&quot;) . [{&#39;translation_text&#39;: &#39;das ist mein Raum.&#39;}] . I probably would have used Zimmer instead of Raum, since Raum is more &quot;space&quot; than &quot;room&quot; to me. . en_to_de_translate(&quot;monkey, hippo, porcupine, dog, cat, rabbit&quot;) . [{&#39;translation_text&#39;: &#39;Affen, Hippo, Pfauen, Hunde, Katzen, Kaninchen, Hunde, Katzen, Kaninchen.&#39;}] . It looks like it uses the plural for nouns. Hippo didn&#39;t translate to anything different, apparently Flusspferd (water horse) is favored by Leo. I like Stachelschwein (&quot;spike pig&quot;) better for porcupine (which apparently live in Texas now!?) and furthermore Pfauen looks to actual mean peacocks. I&#39;m not sure why dog (Hund), cat (Katze), and rabbit (Kaninchen) are repeated, but those look good. . Thus it&#39;s not perfect, but something that took less than a minute can out-translate my 4-ish years of German classes that I haven&#39;t touched up on in like a decade. Ouch. . Named Entity Recognition . Finally, to cap off this short test post let&#39;s try out the named entity recognition task. They provide an example of the classifier in their docs as well as a short list of what different abbreviations mean: . O, Outside of a named entity | B-MIS, Beginning of a miscellaneous entity right after another miscellaneous entity | I-MIS, Miscellaneous entity | B-PER, Beginning of a person’s name right after another person’s name | I-PER, Person’s name | B-ORG, Beginning of an organisation right after another organisation | I-ORG, Organisation | B-LOC, Beginning of a location right after another location | I-LOC, Location | . ner = pipeline(&#39;ner&#39;) . . First using their example: . sequence = (&quot;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&quot; + &quot;close to the Manhattan Bridge which is visible from the window.&quot;) . from pprint import pprint . for entry in ner(sequence): pprint(entry) . {&#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 1, &#39;score&#39;: 0.9995632767677307, &#39;word&#39;: &#39;Hu&#39;} {&#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 2, &#39;score&#39;: 0.9915938973426819, &#39;word&#39;: &#39;##gging&#39;} {&#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 3, &#39;score&#39;: 0.9982671737670898, &#39;word&#39;: &#39;Face&#39;} {&#39;entity&#39;: &#39;I-ORG&#39;, &#39;index&#39;: 4, &#39;score&#39;: 0.9994403719902039, &#39;word&#39;: &#39;Inc&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 11, &#39;score&#39;: 0.9994346499443054, &#39;word&#39;: &#39;New&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 12, &#39;score&#39;: 0.9993270635604858, &#39;word&#39;: &#39;York&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 13, &#39;score&#39;: 0.9993864893913269, &#39;word&#39;: &#39;City&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 19, &#39;score&#39;: 0.9825621843338013, &#39;word&#39;: &#39;D&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 20, &#39;score&#39;: 0.9369831085205078, &#39;word&#39;: &#39;##UM&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 21, &#39;score&#39;: 0.8987104296684265, &#39;word&#39;: &#39;##BO&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 29, &#39;score&#39;: 0.9758240580558777, &#39;word&#39;: &#39;Manhattan&#39;} {&#39;entity&#39;: &#39;I-LOC&#39;, &#39;index&#39;: 30, &#39;score&#39;: 0.9902493953704834, &#39;word&#39;: &#39;Bridge&#39;} . Impressive, especially how it recognizes DUMBO as a location. (side note, I actually visited that area in my first trip to NYC last year). . Looking forward to trying out these transformers more in the future! .",
            "url": "http://simonstolarczyk.com/jupyter/2020/08/15/huggingface_test.html",
            "relUrl": "/jupyter/2020/08/15/huggingface_test.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
  
    
        ,"post4": {
            "title": "Make More Interesting Random Music",
            "content": "Make More Interesting Random Music . We saw last time that it is simple to construct a MIDI file with the pretty_midi package. Now to make something a little more musically interesting than alternating between two notes we need to randomly generate notes to play using some basic music theory to make things sound “good.” To do this we will: . choose a scale | find all of the midi notes attached to that scale | randomly draw notes from that collection of midi notes | get a pleasing collection of simple chords to accompany the melody | vary the rhythm of the melody | . Pick a Scale . First we use a couple of nice utilities of pretty_midi, a method which takes an integer and spits out one of the major and minor keys and another method which takes that same integer and returns how many accidentals the key has. Using the key name we can determine whether the key has flat accidentals or sharp accidentals and then by using the fact that “accidentals accumulate” (if a key has a G# then it also has a C#, for example) we can easily identify which notes are in the key. . To say more, we are using the fact that the Major and Minor keys have certain nice patterns. If we start from C and move up in perfect 5ths (C, G, D, A) then those corresponding (major) keys each add an accidental: . C Major: CDEFGAB | G Major: GABCDEF# | D Major: DEF#GABC# | A Major: ABC#DEF#G# | … | . This means we know the exact notes that are made sharp or (flat) if we know if the key is sharp (flat) and how many accidentals there are, we do not even need to know the root: we are being told the same information in a different way. Now, there is actually a more insightful way to do this (how making major scales is normally taught) by using the fact that you start at the root and add notes with the pattern WWHWWWH, but it was fun to think of a different way to get the notes. . We return the key_notes which runs from C to B because this is how the MIDI format is laid out for each octave (… B1 C2 C#2 … A#2 B2 C3 …), but we also save off the notes of the key starting from the root for (optional) printing to the user as scale_notes. . note_names = [n for n in &#39;CDEFGAB&#39;] sharp_accidentals = [n for n in &#39;FCGDAEB&#39;] flat_accidentals = [n for n in &#39;BEADGCF&#39;] def determine_key_notes(key_number): key_name = pretty_midi.key_number_to_key_name(key_number) _, num_accidentals = pretty_midi.key_number_to_mode_accidentals(key_number) root = key_name[0] root_index = note_names.index(root) if key_name[1] == &quot;b&quot;: accidental_mark = &#39;b&#39; accidentals = flat_accidentals[:num_accidentals] else: accidental_mark = &#39;#&#39; accidentals = sharp_accidentals[:num_accidentals] key_notes = list(map(lambda n: n + accidental_mark if n in accidentals else n, note_names)) scale_notes = (key_notes + key_notes)[root_index:root_index + 7] . Find All MIDI Notes Given Key Notes . For this, use the pretty_midi utility that converts note names to MIDI note numbers and apply it to all the note names with all the octave numbers attached. . def get_all_midi_numbers(note_names): midi_numbers = [] note_names = list(set(note_names)) # simple de-dup for octave in range(-1, 9): for note in note_names: midi_number = pretty_midi.note_name_to_number(note + str(octave)) midi_numbers.append(midi_number) return sorted(midi_numbers) . We do not need to de-duplicate or sort for our current use, but I added those steps in case I supply some note names “out of order” ([D, C] instead of [C, D] for instance) or provide possible note duplicates for other uses. . Randomly Draw Notes . Here you can now use the collection of midi notes and just make a random choice from it at each step. For instance if g_maj_midi is the collection of MIDI notes for G Major then you can randomly select one with pitch = random.choice(g_maj_midi[21:36]) where we take a slice of the array to restrict the notes to just a couple of octaves. . Chord Accompaniment . We want to get just simple chords from the major scale for the piano to play them for whole notes. There are some existing Python packages that can be used: . chords2midi let’s you generate a MIDI file by supplying a progressing an a key: c2m I V vi IV --key C | chords2midi uses pychord, which will generate the component notes of a chord from its name as well as name a chord from its notes. | . These are both strong utilities that I will certainly use as I expand but for now I will do something much simpler. You can get a major scale chord progression by simply going to the scale and taking a note for the root and getting the third and fifth of a triad by just taking the second and fourth notes after your root. For instance, you can get a C (major) triad from the C major scale by looking at the scale CDEFGAB and using that pattern C_E_G__. Whether this is major or minor is irrelevant for our use: we just want to grab all the simple triads (not worrying about inversions or anything) and collect them together for one octave of root notes: . def get_major_progression(root_index, midi_numbers): chords = [] for i in range(8): chord_root_index = root_index + i chord_third_index = chord_root_index + 2 chord_fifth_index = chord_root_index + 4 root = midi_numbers[chord_root_index] third = midi_numbers[chord_third_index] fifth = midi_numbers[chord_fifth_index] chord = [root, third, fifth] chords.append(chord) return chords . This will generate a list of chord lists, where each chord list is the midi notes for a given triad. Note, if you do not feed the notes of a key as the MIDI numbers you will not get a major key progression, so this might be imperfectly named. . Vary the Rhythm . When we were adding notes one at a time we were keeping track of when the note began and when it ended in seconds. It would be nice to forget about when they begin and imagine writing the song and moving forward, adding notes “now.” To do this we make a writer for our instrument that keeps track of when “now” is and allows us to add notes one at a time, as a chord, or in a chunk of notes. Using this chunking allows us to vary the rhythm, we can randomly determine how long the next note(s) should be and then play a bunch of notes of that length. This is slightly more natural than varying each note independently, because shorter notes are often grouped together. . class Writer: def __init__(self, instrument): self.position = 0 self.instrument = instrument def add_note(self, pitch, length, move_forward=True): note = pretty_midi.Note(velocity=100, pitch=pitch, start=self.position, end=self.position + length) self.instrument.notes.append(note) if move_forward: self.position += length def add_note_series(self, pitches, length): for pitch in pitches: self.add_note(pitch, length) def add_chord(self, pitches, length, move_forward=True): for pitch in pitches: self.add_note(pitch, length, move_forward=False) if move_forward: self.position += length . We leave move_forward as an optional argument, because if we are writing notes that will occur at the same time (to form a chord) then we want the Writer “head” or position to stay at the same spot until we are done adding notes to that point in time. There are other simplifications that are made (no velocity changes), but this simple class gives us a lot of power so that we can more expressively generate midi music. . import pretty_midi from music_info import determine_key_notes import music_info import random # Create a PrettyMIDI object ensemble = pretty_midi.PrettyMIDI() # Create an Instrument instance for a cello instrument # Changed to a guitar for my song, and was lazy about changing variable names # cello_program = pretty_midi.instrument_name_to_program(&#39;Cello&#39;) cello_program = pretty_midi.instrument_name_to_program(&#39;Overdriven Guitar&#39;) cello = pretty_midi.Instrument(program=cello_program) # do the same for a piano piano_program = pretty_midi.instrument_name_to_program(&#39;Acoustic Grand Piano&#39;) piano = pretty_midi.Instrument(program=piano_program) # Add the instruments to the PrettyMIDI object ensemble.instruments.append(cello) ensemble.instruments.append(piano) # here is where I put the Writer class from above piano_writer = Writer(piano) cello_writer = Writer(cello) song_length_in_seconds = 30 bpm = 120 beat_length = 60 / bpm num_beats = int(song_length_in_seconds / beat_length) # Decided on G major, which is index 7 g_maj = music_info.determine_key_notes(7) g_maj_midi = music_info.get_all_midi_numbers(g_maj) root_number = pretty_midi.note_name_to_number(&quot;G4&quot;) root_index = g_maj_midi.index(root_number) major_progression = music_info.get_major_progression(root_index, g_maj_midi) accompaniment_writer = piano_writer solo_writer = cello_writer # now we see the power of the writer object while accompaniment_writer.position &lt; song_length_in_seconds: length = beat_length * 4 # whole note chords # choose a random chord from the progression chord = random.choice(major_progression) # add a lower octave of the notes for fullness larger_chord = chord + [n - 12 for n in chord] # write the chord accompaniment_writer.add_chord(larger_chord, length) while solo_writer.position &lt; song_length_in_seconds: # choose to play 16th, 8th, quarter, half, or whole note(s) division = random.choice([-2, -1, 0, 1, 2]) length = beat_length * (2 ** division) # if we chose 16th or 8th, play multiple of them if division &lt; 0: num_notes = 2 ** (-1 * division) pitches = random.choices(g_maj_midi[21:36], k=num_notes) solo_writer.add_note_series(pitches, length) else: pitch = random.choice(g_maj_midi[21:36]) solo_writer.add_note(pitch, length=beat_length) def write_song(): # Write out the MIDI data ensemble.write(&#39;duo.mid&#39;) . Now with this I created the following simple duet. It isn’t the most thrilling piece of music, but with relatively simple rules behind it, I think it is interesting how “complex” it sounds. .",
            "url": "http://simonstolarczyk.com/markdown/2020/08/02/Python_and_MIDI_2.html",
            "relUrl": "/markdown/2020/08/02/Python_and_MIDI_2.html",
            "date": " • Aug 2, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Making Music with pretty_midi",
            "content": "Making Music with pretty_midi . MIDI Music . A while back, when I was first learning Python, a friend and I made a program that generated random music by iterating through a song one note at a time randomly picking the note length and pitch for the notes with increasingly strict rules. My friend figured out how to use MIDIUtil to create a midi file by writing down a sequence of note events, where each event says something about: . velocity - how loud the note should be played | pitch - an integer value for telling a midi play what frequency to play | note start - when the note should start playing | note end - when it should stop | . You can do a lot of music with just that information, and if you make simple choices for notes, with the right MIDI player you can get surprisingly listenable music. I have wanted to expand on this program for a while in various ways: . generating music by applying machine learning techniques to various collections of midi files (like this Google Bach doodle) | making something more interactive (something involving the console where you can add motifs and ideas on the play and have them played back) | feed the program just a text file with a simplistic music notation to easily create ideas that can get more complex (like TidalCycles which uses Haskell and SuperCollider). | . Creating MIDI Files with Python . I searched around on pypi a bit and found a promising package to start writing MIDI files with: pretty_midi. It’s a project on GitHub that seems to still be active (part of why I am not just using MIDIUtil again is that I remember I slightly annoying setup and it seems to be inactive). After a simple install, and running the second example from the documentation I found that it easily generated pleasing sounds that (after a bit of fitzing) I could simply listen to with VLC Media Player. Before I just imported the files into Reaper where I could use a custom VST, but this makes iterating a bit quicker. . Here is an example I made where two instruments are playing a very minimalist piece: . import pretty_midi # Create a PrettyMIDI object ensemble = pretty_midi.PrettyMIDI() # Create an Instrument instance for a cello instrument cello_program = pretty_midi.instrument_name_to_program(&#39;Cello&#39;) cello = pretty_midi.Instrument(program=cello_program) # do the same for a piano piano_program = pretty_midi.instrument_name_to_program(&#39;Acoustic Grand Piano&#39;) piano = pretty_midi.Instrument(program=piano_program) # Add the instruments to the PrettyMIDI object ensemble.instruments.append(cello) ensemble.instruments.append(piano) song_length_in_seconds = 30 bpm = 120 beat_length = 60 / bpm num_beats = int(song_length_in_seconds / beat_length) for beat in range(num_beats): if beat % 2 == 0: note_name = &#39;C5&#39; else: note_name = &#39;D5&#39; note_number = pretty_midi.note_name_to_number(note_name) note_start = beat * beat_length note_end = note_start + beat_length note = pretty_midi.Note(velocity=100, pitch=note_number, start=note_start, end=note_end) cello.notes.append(note) piano.notes.append(note) def write_song(): # Write out the MIDI data ensemble.write(&#39;ensemble.mid&#39;) if __name__ == &quot;__main__&quot;: write_song() . This is a simple example, yet it shows the basics of the process that I will expand on: . add notes one at a time | incorporate some randomness into the note properties (here pitch changes deterministically, but that will soon change) | using multiple instruments | . I will begin to add to this, hopefully reaching what we achieved during my first experience with Python and MIDI and then think about ways to expand it and then start to apply some of my fastbook reading to it. .",
            "url": "http://simonstolarczyk.com/markdown/2020/08/01/Python_and_MIDI.html",
            "relUrl": "/markdown/2020/08/01/Python_and_MIDI.html",
            "date": " • Aug 1, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "DataPlox",
            "content": "DataPlox . In understanding how to properly feed young, growing models, it is useful to know some of the main data objects in PyTorch and fastai. Fastai extends many of the fundamental PyTorch objects. Here’s the picture that I have in my head: . . For the main PyTorch data classes: . Dataset | DataLoader | . The fastai library extends them with: . Datasets | DataLoaders | . and adds a useful class which can help construct the others: . DataBlock | . DataBlock . This is a fastai object which helps you build Datasets and DataLoaders. In addition to Chapter 6 of the fastbook, there is also a tutorial in the fastai docs. A DataBlock is a blueprint for how to build your data. You tell it: . what kind of data you have (blocks=), | how to get the input data (get_x= or get_items=), | how to get the targets/labels (get_y), | how to perform the train/validation split (splitter=), | as well as any resizing (items_tfms=) or augmentations you want to be performed (batch_tfms). | . By feeding these blue prints a source (like a directory on your computer), you can use a DataBlock to create a Datasets or DataLoaders object. . Dataset . Dataset is a Torch object. We can find out exactly what a Dataset is because PyTorch is open source. We just have to be brave enough to parse some of the grittier implementation details. . According to the source code a Dataset is at its core something that allows you to grab an item if you provide the index/key for it and that you can also add items to. This is just the abstract class definition, essentially the bare bones of a what a dataset should be. If you try to make a class that inherits from Dataset you will get an error if you do not implement __getitem__, the method for grabbing items. It does this by setting the default behavior of that method to raise a NotImplementedError. You can also implement this behavior (forcing inheriting classes to define specific methods) by using the abc package. The source code also mentions that it would have set a default for a length function, but the standard methods for making a default that is forced to change have conflicts with what a length function is “supposed” to do. . The types of Datasets are: . IterableDataset | TensorDataset | ConcatDataset | ChainDataset | Subset | . Datasets . Datasets is an object that contains a training Dataset and a validation Dataset. You can generally construct a Datasets object from a DataBlock like this: . dblock = DataBlock(blue_print_details) dsets = dblock.datasets(source) . DataLoader . A DataLoader is a Dataset together with a Sampler. A Sampler is a way to create an iterator out of your Dataset, so you can do things like consume data in batches as needed. Rather than take a Dataset an manually loop through chunks of it, at each step using a chunk to update a model, a DataLoader bundles this idea together. This makes a lot of sense to encapsulate: going through your data in batches is a frequently encountered process in machine learning! . We can see from the source code that a Sampler is at minimum: . a way to iterate over indices of dataset elements (__iter__) and | a way to calculate the size of the returned iterators (__len__). | . Just like with DataSet, defining the length method is not strictly enforced by the interpreter because the various NotImplemented errors you can throw do not quite work.` . The kinds of samplers: . SequentialSampler - go in direct 0, 1, 2, … order. | RandomSampler - randomly choose observations, with or without replacement (replacement=) | SubsetRandomSampler - randomly sample from a provided subset of indices, without replacement | WeightedRandomSampler - for non-uniform random sampling | BatchSampler - generate mini-batches of indices | . For DataLoader, the definition is a bit more involved. In part, because it implements multiprocessing, but it also does things like creating a Sampler from the arguments if one wasn’t provided. . DataLoaders . DataLoaders is an object that contains a training DataLoader and a validation DataLoader. You can construct a DataLoaders from a DataBlock similarly to the Datasets method: . dblock = DataBlock(blue_print_details) dls = dblock.dataloaders(source) .",
            "url": "http://simonstolarczyk.com/markdown/2020/07/23/data_objects.html",
            "relUrl": "/markdown/2020/07/23/data_objects.html",
            "date": " • Jul 23, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "FastBook Chapter 5 Thoughts",
            "content": "FastBook Chapter 5 Thoughts . After reading the first half of chapter 3 (will read the second half this week) and (mostly) breezing through Chapter 4 (it contained a lot of familiar material), I worked on Chapter 5 this weekend. . Import concepts: . Presizing. | Checking your DataBlock before you begin training | Train early (get a reasonable MVP) and often (if it’s not too expensive). | Cross-Entropy Loss for the binary case and extending it to multi-class examples. | Confusion matrix with ClassificationInterpretation and looking at the most_confused examples. | Learning Rate Finder | More particulars on transfer learning, including how to use discriminative learning rates to not lose the solid training of the transferred modeled. | . Some Notes During Reading . Presizing is a particular way to do image augmentation that is designed to minimize data destruction while maintaining good performance. The general idea is to apply a composition of the augmentation operations all at once rather than iteratively augment and interpolate. This has savings both in terms of computation and the final quality of the examples. . In the 3s and 7s table there is a column labeled “loss”, which for me was a bit confusing. In the first row loss was the predicted output of the “3” class, which happened to be the correct answer. However, loss was just the output of that example, which does not quite make sense because you are looking to minimize loss which conflicts with the goal of maximizing the predicted output for the true class. It looks like this was just an oversight with the naming convention because to compute the loss more things are done and the text that follows makes this clear. . I found it useful to explicitly calculate the loss in the binary example provided. . Activations: . acts[0, :] . tensor([0.6734, 0.2576]) . class0_act = acts[0, 0] class1_act = acts[0, 1] class0_act . tensor(0.6734) . Computing the exponential of the activations to then get the softmax. . from math import exp exp0 = exp(class0_act) exp0 . 1.9608552547588787 . exp1 = exp(class1_act) smax0 = exp0 / (exp0 + exp1) smax0 . 0.602468670578454 . smax1 = exp1 / (exp0 + exp1) smax1 . 0.39753132942154595 . smax0 + smax1 . 1.0 . from math import log log(smax0) . -0.5067196140092344 . log(smax1) . -0.9224815318387478 . -log(smax0) . 0.5067196140092344 . And that is the loss for the first example, because the true class was 0. This matches the calculation using the fastai classes, which is always a relief. . References to Read . Cyclical Learning Rates for Training Neural Networks | How transferable are features in deep neural networks? | . Thoughts on selected Qs . Why do we first resize to a large size on the CPU, and then to a smaller size on the GPU? . You want to create a uniform input size for your data and also apply various transformations to augment it. The presizing method, running augmentation transformations as a single composition rather than iteratively, allows you to have larger/more “rich” inputs to transform before making them a smaller, uniform size that you will train the model with. . | What are the two ways in which data is most commonly provided, for most deep learning datasets? . A collection of data elements, that have filenames indicating information about them, like their class. (A folder of pictures where each picture has a file name with its ID and class). | Tabular format that can either contain the data in each row (along with the metadata) or point to data in other formats. (A csv file with ID, true class, and a hyper link to the input picture.) | | Look up the documentation for L and try using a few of the new methods is that it adds. . L is a beefier list class. How it’s different from a regular list: . the print function is smarter. It provides the length of the list and truncates the end, which is nice if you’ve ever crashed a server because you accidentally printed out an obscenely long list. | you can access L with a tuple, whereas a normal list will break if you try to access it that way. | it has unique(), which functions like the same method in Pandas. | it has a filter method attribute. | . | Look up the documentation for the Python pathlib module and try using a few methods of the Path class. . Path was introduced to Python in 3.4. It appears to combine a bunch of common things that you typically use os with along with the ability to manage file paths without doing string manipulations (as well as reducing the vs / mistakes that are frequently made). . One nice thing that can be done, set here = Path(&#39;.&#39;) and then iterate over the current directory with for f in here.iterdir(): print(f). You can also .open() a path object rather than feeding it to open() and do glob stuff. . | Give two examples of ways that image transformations can degrade the quality of the data. . Image simply rotating a square 45 degrees to stand it up on one corner. Now the new image that you get (take an old square position cut out of the rotated square position) is missing anything in the corners, so it has to be interpolated. This loses about 17% of the original image, so it’s pretty significant! | Brightening an image will move the brighter pixels up to the maximum brightness, so their original brightness cannot be recovered by simply redarkening. | | What method does fastai provide to view the data in a DataLoaders? . You can use .show_batch(nrows, ncols) on the DataLoaders object to get a grid of some of the examples. . | What method does fastai provide to help you debug a DataBlock? . Using .summary() on the DataBlock object gives a verbose attempt to try and create the batch. The output from this, along with errors that come up if it fails can help you notice a problem. . | Should you hold off on training a model until you have thoroughly cleaned your data? . No, sometimes life is easy! Also, it’s good to get reasonable bench marks as soon as possible. Not only to help game-ify the problem and motivate you to work on it, but also to have a baseline to see if the room for improvement is worth the energy. . | What are the two pieces that are combined into cross-entropy loss in PyTorch? . nn.CrossEntropyLoss (see the docs) applies nll_loss after log_softmax (which is log of softmax) . | What are the two properties of activations that softmax ensures? Why is this important? . You can interpret activations as probabilities. outputs sum to one | outputs are non-negative | . | It forces the model to favor a single class. | This more relevant behavior that is mentioned that it amplifies small differences, which is useful if you want the network to be somewhat decisive rather than having all outputs close to each other. . | When might you want your activations to not have these two properties? . The parenthetical comment in the main text mentions that you may not want the model to pick a class just because it has a slightly larger output. You want the model to be sure about the class, not just relatively sure. . For the probability property, it might be misleading because it isn’t necessarily the actual probability of the example being that class. . | Why can’t we use torch.where to create a loss function for datasets where our label can have more than two categories? . In part, this is a constraint of the where function. Where selects between two outputs based on a condition. It is too difficult to right a nested condition when you have more than two outcomes and selecting the loss requires a bit more work, so this trick becomes way less convenient. . | What are two good rules of thumb for picking a learning rate from the learning rate finder? . One order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10). | The last point where the loss was clearly decreasing | . | What two steps does the fine_tune method do? . We go to the source: . self.freeze() self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs) base_lr /= 2 self.unfreeze() self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs) . Thus it: 1. Performs a one-cycle fit with the pre-trained layers frozen (their weights do not update). 2. Performs another one-cycle fit with the pre-trained layers unfrozen at half the learning rate. . | What are discriminative learning rates? . The idea here is that you may want weights in certain layers to change at a different rate. In particular, if your first layers come from a pretrained network you may want to do updates to them more slowly than the last layers which are tailored to your particular problem. . | How is a Python slice object interpreted when passed as a learning rate to fastai? . It acts like numpy.linspace where the num is implicitly defined as the number of layers. . | Why is early stopping a poor choice when using 1cycle training? . For a description of 1cycle training, the fastai docs refer to the rate finder paper in the references as well as this blog post. It looks like the basic idea is stopping early does give the training a chance to be finely tuned, because you are likely stopping at a point where the learning rate is still large. . | What is the difference between resnet50 and resnet101? . Both resnet50 and resnet100 are residual networks, and seem to have been introduced in Deep Residual Learning for Image Recognition. The basic idea of deep residual networks seems to be “wire a network that is trying to learn the function $ mathcal{H}(x)$ such that it has to learn $ mathcal{H}(x) - x$ instead.” The intuition being that, for example, it is easier to learn the zero function than it is to learn the identity function. resnet-50 looks like it was obtain from the resnet-34 architecture by replacing certain layer pairs with layer triplets known as bottleneck blocks. resnet-101 (and resnet-152) are just an expansion of this idea, adding 17 more (or 34 more) of these triplet-layer blocks. . | What does to_fp16 do? . The other downside of deeper architectures is that they take quite a bit longer to train. One technique that can speed things up a lot is mixed-precision training. This refers to using less-precise numbers (half-precision floating point, also called fp16) where possible during training. As we are writing these words in early 2020, nearly all current NVIDIA GPUs support a special feature called tensor cores that can dramatically speed up neural network training, by 2-3x. They also require a lot less GPU memory. To enable this feature in fastai, just add to_fp16() after your Learner creation (you also need to import the module). . |",
            "url": "http://simonstolarczyk.com/markdown/2020/07/21/fastbook_ch5.html",
            "relUrl": "/markdown/2020/07/21/fastbook_ch5.html",
            "date": " • Jul 21, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Solving a Nightmare with a Headache",
            "content": "Solving a Nightmare with a Headache . Issue: The widgets in Jupyter Notebook were not all displaying. . Non-solution: Try re-do some installs in the virtualenv. While I am at it, let’s also try out pyenv. . New Issue: Windows does not play well with pyenv. You can get pyenv to work, but then virtualenv is no longer set up on your global environment. . Attempted Solution: Try to clone the pyenv-virtualenv repo into a new plugins directory of the pyenv root folder. This did not work, despite sinking about an hour into this whole mess. . New Issue: It’s time to put Linux on my machine to try and escape some of these Windows nightmares. This is going to be a headache, but it seems like the pay off will be worth-while, given that the Fastbook also doesn’t play well with Windows. I have been frustrated with doing Python things on Windows before so I had Linux available already. I will note that it took be a while to remember I already had a Linux partition and in a cascading wave of failure, I forgot my linux password, so was not able to run a needed sudo command. But, I got lucky again (and this is a yikes security-wise) and it turns out you can easily reset the password for Ubuntu. . From here I followed Real Python’s primer on pyenv. Setting this up was not bad and putting some lines in .bashrc virtualenv becomes much nicer to use. It allows you connect a particular environment to a folder. I created a virtualenv named fab (FastAI Book, short environment names are good when you’re used to activating an environment every time you want to work on something) by again following Real Python and now it is automatically activated when I am in the fastbook directory. A quick pip install -r requirements.txt -v got the environment running smoothly with the notebook for Chapter 1, and things that did not work 100% in Windows worked in my Linux boot nicely (at least after a sudo apt install graphviz): . I could run the cells without having to change num_workers to 0. | My GPU was visible to Torch (torch.cuda.get_device_name(0) returned the name of my GPU). | The text example, which did not work at all on Windows, ran. | And, the whole reason I started this endeavor: when I created the FileUploader widget, it appeared in the notebook. | .",
            "url": "http://simonstolarczyk.com/markdown/2020/07/12/wind_nightmare.html",
            "relUrl": "/markdown/2020/07/12/wind_nightmare.html",
            "date": " • Jul 12, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "FastAI Book Chapter 2",
            "content": "FastAI Book Chapter 2 . I went through the second chapter of the book today, which is why this blog even exists. Highly useful; looking forward to working on some music projects to really learn the material. . Things I Learned . How to easily build an image classifier to discern between blue jays, mockingbirds, and shrikes. | . I chose the first two bird types based on my familiarity and their Cool Local Bird ranks. I was going to select a cardinal as a third class, but thought it would be too easy to detect the class based solely on color, so I searched around and found shrikes, which looked similar to mockingbirds. The classifier performed very well, which was I semi-shock because I didn’t know: . You do not always need a lot of data to build a decent model. | . Even with 150 examples of each of three classes (before train/validation split), there were only 3 misclassifications on the validation set. This shows how powerful data augmentation can be, but also helps dispel the notion that you need tons of data to do anything reasonable. . FastAI has some powerful tools that I need to learn. | . Easily display the examples with the highest loss and lowest confidence to see if I can interpret possible deficiencies with interp.plot_top_losses(). | Clean up the dataset with ImageClassifierCleaner, manually going through some of the examples to change labels or remove them, and then effect the changes with a couple simple for-loops. | Use verify_images to clean up corrupted files easily. | Use DataBlock to define the structure of the problem and implement useful transformations for the data. | Think about how to normalize images. | . Squashing a large image into a smaller one with simple scaling or adding cropped parts of small images is maybe not the best way to make sure the inputs are all “from the same distribution”. You can use RandomResizedCrop to maintain original image quality and also artificially expand the size of your training set. . Other things: How to easily build a dataset with the Bing Image API. | The usefulness of Path (which I only recently learned about) was really demonstrated nicely. | The Drivetrain Approach. | Creating a Notebook App with widgets and Voilà. (Admittedly, I’m still trying to get the latter to work.) | . | . Things I Had to Troubleshoot . How do you find the Bing Image Search key? | . Solution: I signed up for the free Azure thing, went to to the Bing Image Search API, and just had to click to add Bing Search APIs v7 to my subscription. Once I did this it brought up a page with the keys and endpoints. . I am using my local computer, with Windows, to run the notebooks. So, I have run into (standard) issues. Namely, I saw RuntimeError: cuda runtime error (801) : operation not supported at... when I first attempted to fine tune the learner learn.fine_tune(4). | . Solution: When you google the error this issue page points you to the forum, but also usefully mentions the “need to set num_workers=0 when creating a DataLoaders because Pytorch multiprocessing does not work on Windows.” So, doing this when you define the dataloaders dls = bears.dataloaders(path, num_workers=0) cleared it up for me. . Some minor formatting issues with interp.plot_top_losses(5, nrows=1). The text above the images was overlapping, because my class names were a bit long. | . Simple fix was to set nrows=5. .",
            "url": "http://simonstolarczyk.com/markdown/2020/07/11/fastai_book_ch2.html",
            "relUrl": "/markdown/2020/07/11/fastai_book_ch2.html",
            "date": " • Jul 11, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m currently a data scientist at an insurance company, focusing on all things graph: graph databases, network analysis, and GNNs. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "http://simonstolarczyk.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://simonstolarczyk.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}